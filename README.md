<!-- ## A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond -->

## Neural Code Intelligence Survey
This is the repository of our paper: **A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond**.

 <!-- [[Paper](https://qiushisun.github.io/)] -->

[![arXiv](https://img.shields.io/badge/arXiv-2403.14734-b31b1b.svg)](https://arxiv.org/abs/2403.14734) 
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) 
[![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](http://makeapullrequest.com)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![License](https://img.shields.io/badge/License-MIT-blue)
[![Paper page](https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-sm.svg)](https://huggingface.co/papers/2403.14734)
[![Twitter Follow](https://img.shields.io/twitter/follow/qiushi_sun)](https://twitter.com/qiushi_sun)
<!-- [ü§ó](https://huggingface.co/papers/2403.14734) -->

*Please do not hesitate to contact us or launch pull requests if you find any related papers that are missing in our paper.*

## News üì∞
- Update on 2025/01/26: Version 1.6 released with additional topics on code preference learning üìñ
- Update on 2024/11/01: Version 1.4 released with extra paper collections üìñ
- Update on 2024/06/23: Version 1.2 released üöÄ
- Update on 2024/03/19: Version 1.0 released üöÄ
- Update on 2024/01/19: Add multiple reading lists üìñ
- Update on 2023/12/29: Add Development Timelines üìÖ
- Update on 2023/12/25: Add Reading Lists, Merry Christmas üçéüéÑ

## Introduction üìú

üìÉ [**A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond**](https://arxiv.org/abs/2403.14734) 
>
> [Qiushi Sun](https://qiushisun.github.io/),
[Zhirui Chen](https://github.com/jet1004),
[Fangzhi Xu](https://xufangzhi.github.io/),
[Kanzhi Cheng](https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-CN),
[Chang Ma](https://chang-github-00.github.io/-changma/),
[Zhangyue Yin](https://scholar.google.com/citations?user=9gRQqSkAAAAJ&hl=en),
[Jianing Wang](https://wjn1996.github.io/),
[Chengcheng Han](https://hccngu.github.io/),
[Renyu Zhu](https://scholar.google.com/citations?user=tSWULnAAAAAJ&hl=en), 
[Shuai Yuan](https://github.com/Luciferder),
[Qipeng Guo](https://scholar.google.com/citations?user=k3mPGKgAAAAJ&hl=en),
[Xipeng Qiu](https://xpqiu.github.io/),
[Pengcheng Yin](https://pengcheng.in/),
[Xiaoli Li](https://www.a-star.edu.sg/i2r/about-i2r/i2r-management/li-xiaoli), 
[Fei Yuan](https://github.com/CONE-MT), 
[Lingpeng Kong](https://ikekonglp.github.io/), 
[Xiang Li](https://lixiang3776.github.io/), 
[Zhiyong Wu](https://lividwo.github.io/zywu.github.io/)

Introducing the resources provided by our survey paper, slides is also available at [here](assets/NCI_Survey_Slides_V1.pdf).

## Timeline

<details open>
<summary>The Development of Code Intelligence</summary>

![milestones](assets/codelms-tree-v18.png)

</details>


## Recent Work on Code Intelligence (Welcome PR) üìó


- [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316) 2025.2
- [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807) 2025.2
- [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694v1) 2025.1
- [WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models](https://arxiv.org/abs/2412.17395) 2024.12
- [FullStack Bench: Evaluating LLMs as Full Stack Coders](https://arxiv.org/abs/2412.00535) 2024.12
- [CodeDPO: Aligning Code Models with Self Generated and Verified Source Code](https://arxiv.org/abs/2410.05605) 2024.11
- [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://opencoder-llm.github.io/) 2024.11
- [Qwen2.5-Coder Series: Powerful, Diverse, Practical.](https://github.com/QwenLM/Qwen2.5-Coder) 2024.11
<!-- - [RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning](https://huggingface.co/papers/2410.02089) 2024.10 -->
<!-- - [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914) 2024.08 -->
  

<!-- - [Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](https://arxiv.org/abs/2406.11736) 2024.06
- [Symbolic Learning Enables Self-Evolving Agents](https://arxiv.org/pdf/2406.18532v1) 2024.06
- [aiXcoder](https://huggingface.co/aiXcoder/aixcoder-7b-base) 2024.04
- [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) 2024.03 -->


## Paper Collections / Tutorials üìö

- [Language Models for Code](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/CodeLMs.md) ü§ñ
- [Evaluations and Benchmarks](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Benchmarks.md) üìä 
- [Preference Optimization](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Preference-Optimization.md) üçé
- [Code Repair](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Repair.md) üîß
- [Reasoning with Code Synthesis](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Reasoning.md) üß†
- [Data Science](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/DS.md) üî¢
- [Corpus containing Code Data](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Code-corpus.md) üìö
- [Code-Based Solutions for NLP Tasks](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/NLPTasks-through-code.md) üìù
- [Code Empowered Agents](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/CodeLM-empowered-agents.md) ü§ñ
- [Reinforcement Learning with CodeLMs](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/RL-with-CodeLMs.md) üéÆ
- [Code Intelligence assists AI4Science](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/AI4Science.md) üß™
- [Software Development](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Software-Development.md) üõ†Ô∏è
- [Multilingual](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/multilingual.md) üåç
- [Multimodal Code Generation](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Multimodal.md) üé®
- [Awesome Slides, Talks and Blogs](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/tutorials.md) üßë‚Äçüè´

## Citation üìñ

ü´∂ If you are interested in our work or find this repository helpful, please consider using the following citation format when referencing our paper:

```bibtex
@article{sun2024survey,
  title={A survey of neural code intelligence: Paradigms, advances and beyond},
  author={Sun, Qiushi and Chen, Zhirui and Xu, Fangzhi and Cheng, Kanzhi and Ma, Chang and Yin, Zhangyue and Wang, Jianing and Han, Chengcheng and Zhu, Renyu and Yuan, Shuai and others},
  journal={arXiv preprint arXiv:2403.14734},
  year={2024}
}
```
## Table of Contents

1. [Language Models for Code](#CodeLMs)


## Language Models for Code{#CodeLMs}

### CodeLLMs / Decoder-Only

1. [Preprint] **Qwen2.5-Coder Series: Powerful, Diverse, Practical.** [[Website]](https://github.com/QwenLM/Qwen2.5-Coder), 2024.11

2. [Preprint] **OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models** [[Website]](https://opencoder-llm.github.io/) [![arXiv](https://img.shields.io/badge/arXiv-2411.04905-b31b1b.svg)](https://arxiv.org/abs/2411.04905), 2024.11

   *aiXcoder Contributors*

3. [[COLM2024]](https://openreview.net/forum?id=kWnlCVcp6o#discussion) **Crystal: Illuminating LLM Abilities on Language and Code** [![arXiv](https://img.shields.io/badge/arXiv-2411.04156-b31b1b.svg)](https://arxiv.org/abs/2411.04156), 2024.10

   *aiXcoder Contributors*

4. [Preprint] **aiXcoder** [[Website]](https://github.com/aixcoder-plugin/aiXcoder-7B)

   *aiXcoder Contributors*


1. [Preprint] **CodeGeeX4: Open Multilingual Code Generation Model.** [[Repo]](https://github.com/THUDM/CodeGeeX4), 2024.07

   *Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang*


1. [Preprint] **CodeGemma: Open Code Models Based on Gemmaa.** [![arXiv](https://img.shields.io/badge/arXiv-2406.11409-b31b1b.svg)](https://arxiv.org/abs/2406.11409), 2024.06

   *CodeGemma Team*

2. [Preprint] **Codestral** [[Website]](https://mistral.ai/news/codestral/), 2024.05

   *The Mistral AI Team*

3. [Preprint] **Code with CodeQwen1.5** [[Blog Post]](https://qwenlm.github.io/blog/codeqwen1.5/), 2024.04

   *Qwen Team*

4. [Preprint] **StarCoder 2 and The Stack v2: The Next Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2401.14196-b31b1b.svg)](https://arxiv.org/abs/2402.19173), 2024.02

   *Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries*

5. [Preprint] **DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence.** [![arXiv](https://img.shields.io/badge/arXiv-2401.14196-b31b1b.svg)](https://arxiv.org/abs/2401.14196), 2024.01

   *Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang*

6. [Preprint] **Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models.** [![arXiv](https://img.shields.io/badge/arXiv-2311.09278-b31b1b.svg)](https://arxiv.org/abs/2311.09278), 2023.11 [![zhihu](https://img.shields.io/badge/dynamic/json?label=upvotes&style=social&logo=zhihu&query=$.voteup_count&url=https://www.zhihu.com/api/v4/articles/690058207)](https://zhuanlan.zhihu.com/p/690058207)

   *Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu*

7. [[ICLR2024]](https://openreview.net/forum?id=4WnqRR915j) **Llemma: An Open Language Model For Mathematics.** [![arXiv](https://img.shields.io/badge/arXiv-2310.10631-b31b1b.svg)](https://arxiv.org/abs/2310.10631), 2023.10

   *Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck*

8. [[ICLR2024]](https://openreview.net/forum?id=hNhwSmtXRh) **Lemur: Harmonizing Natural Language and Code for Language Agents.** [![arXiv](https://img.shields.io/badge/arXiv-2310.06830-b31b1b.svg)](https://arxiv.org/abs/2310.06830), 2023.10

   *Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu*

9. [Preprint] **ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.** [![arXiv](https://img.shields.io/badge/arXiv-2309.17452-b31b1b.svg)](https://arxiv.org/abs/2309.17452), 2023.09

   *Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen*

10. [Preprint] **MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning.** [![arXiv](https://img.shields.io/badge/arXiv-2309.05653-b31b1b.svg)](https://arxiv.org/abs/2309.05653), 2023.09

    *Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen*

11. [Preprint] `phi-1.5` **Textbooks Are All You Need II: phi-1.5 technical report.** [![arXiv](https://img.shields.io/badge/arXiv-2309.05463-b31b1b.svg)](https://arxiv.org/abs/2309.05463), 2023.09

    *Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee*

12. [Preprint] **Code Llama: Open Foundation Models for Code.** [![arXiv](https://img.shields.io/badge/arXiv-2308.12950-b31b1b.svg)](https://arxiv.org/abs/2308.12950), 2023.08

   *Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve*

11. [Preprint] `OctoCoder` **OctoPack: Instruction Tuning Code Large Language Models.** [![arXiv](https://img.shields.io/badge/arXiv-2308.07124-b31b1b.svg)](https://arxiv.org/abs/2308.07124), 2023.08

   *Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre*    

12. [Preprint] **PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback.** [![arXiv](https://img.shields.io/badge/arXiv-2307.14936-b31b1b.svg)](https://arxiv.org/abs/2307.14936), 2023.07

    *Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, Qianxiang Wang* 

13. [Preprint] `phi-1` **Textbooks Are All You Need.** [![arXiv](https://img.shields.io/badge/arXiv-2306.11644-b31b1b.svg)](https://arxiv.org/abs/2306.11644), 2023.06
    *Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li*

14. [Preprint] **WizardCoder: Empowering Code Large Language Models with Evol-Instruct.** [![arXiv](https://img.shields.io/badge/arXiv-2306.08568-b31b1b.svg)](https://arxiv.org/abs/2306.08568), 2023.06
    *Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang*

15. [[TMLR]](https://openreview.net/forum?id=KoFOg41haE) **StarCoder: may the source be with you!** [![arXiv](https://img.shields.io/badge/arXiv-2305.06161-b31b1b.svg)](https://arxiv.org/abs/2305.06161), 2023.05

    *DRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries*

16. [[Preprint]](https://openreview.net/forum?id=NRhaCyPhQbm) **CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.** [![arXiv](https://img.shields.io/badge/arXiv-2303.17568-b31b1b.svg)](https://arxiv.org/abs/2303.17568), 2023.03

    *Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang* 

17. [Preprint] **SantaCoder: don't reach for the stars!** [![arXiv](https://img.shields.io/badge/arXiv-2301.03988-b31b1b.svg)](https://arxiv.org/abs/2301.03988), 2023.01

    *Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra*   

18. [Preprint] `FIM` **Efficient Training of Language Models to Fill in the Middle.** [![arXiv](https://img.shields.io/badge/arXiv-2207.14255-b31b1b.svg)](https://arxiv.org/abs/2207.14255), 2022.07

    *Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, Mark Chen*   

19. [Preprint] **PanGu-Coder: Program Synthesis with Function-Level Language Modeling.** [![arXiv](https://img.shields.io/badge/arXiv-2207.11280-b31b1b.svg)](https://arxiv.org/abs/2207.11280), 2022.07

    *Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, Qun Liu*

20. [[IJCAI2022]](https://www.ijcai.org/proceedings/2022/329) `PyCodeGPT` **CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation** [![arXiv](https://img.shields.io/badge/arXiv-2206.06888-b31b1b.svg)](https://arxiv.org/abs/2206.06888), 2022.06

    *Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou*   

21. [[ICLR2023]](https://openreview.net/forum?id=hQwb-lbM6EL) **InCoder: A Generative Model for Code Infilling and Synthesis.** [![arXiv](https://img.shields.io/badge/arXiv-2204.05999-b31b1b.svg)](https://arxiv.org/abs/2204.05999), 2022.04

    *Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis*

22. [[JMLR]](https://jmlr.org/papers/v24/22-1144.html) `PaLM-Coder` **PaLM: Scaling Language Modeling with Pathways.** [![arXiv](https://img.shields.io/badge/arXiv-2204.02311-b31b1b.svg)](https://arxiv.org/abs/2204.02311), 2022.04

    *Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel*

23. [[ICLR2023]](https://openreview.net/forum?id=iaYcJKpY2B_) **CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis.** [![arXiv](https://img.shields.io/badge/arXiv-2203.13474-b31b1b.svg)](https://arxiv.org/abs/2203.13474), 2022.03

    *Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong*  

24. [[MAPS2022]](https://dl.acm.org/doi/abs/10.1145/3520312.3534862) `PolyCoder` **A Systematic Evaluation of Large Language Models of Code.** [![arXiv](https://img.shields.io/badge/arXiv-2202.13169-b31b1b.svg)](https://arxiv.org/abs/2202.13169), 2022.02

    *Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn*

25. [Preprint] `Code-davinci-002` **Evaluating Large Language Models Trained on Code.** [![arXiv](https://img.shields.io/badge/arXiv-2107.03374-b31b1b.svg)](https://arxiv.org/abs/2107.03374), 2021.7

    *Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba* 

26. [[ESEC/FSE2020]](https://2020.esec-fse.org/details/esecfse-2020-industry-papers/13/IntelliCode-Compose-Code-Generation-using-Transformer) `GPT-C` **IntelliCode Compose: Code Generation using Transformer.** [![arXiv](https://img.shields.io/badge/arXiv-2005.08025-b31b1b.svg)](https://arxiv.org/abs/2005.08025), 2020.05

    *Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan*   

### Encoder-Decoder

1. [[EMNLP2023]](https://aclanthology.org/2023.emnlp-main.68/) **CodeT5+: Open Code Large Language Models for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2305.07922-b31b1b.svg)](https://arxiv.org/abs/2305.07922), 2023.05

   *Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi* 

2. [[ACL2023]](https://aclanthology.org/2023.findings-acl.676/) **ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages.** [![arXiv](https://img.shields.io/badge/arXiv-2212.06742-b31b1b.svg)](https://arxiv.org/abs/2212.06742), 2022.12

   *Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu* 

3. [[NeurIPS2022]](https://openreview.net/forum?id=WaGvb7OzySA) **CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning.** [![arXiv](https://img.shields.io/badge/arXiv-2207.01780-b31b1b.svg)](https://arxiv.org/abs/2207.01780), 2022.07

   Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi

4. [[ESEC/FSE]](https://dl.acm.org/doi/abs/10.1145/3540250.3549162) **NatGen: Generative pre-training by "Naturalizing" source code.** [![arXiv](https://img.shields.io/badge/arXiv-2206.07585-b31b1b.svg)](https://arxiv.org/abs/2206.07585), 2022.06

   *Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu, Baishakhi Ray* 

5. [[ACL2022]](https://aclanthology.org/2022.acl-long.499/) **UniXcoder: Unified Cross-Modal Pre-training for Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2203.03850-b31b1b.svg)](https://arxiv.org/abs/2203.03850), 2022.03

   *Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin* 

6. [[Science]](https://www.science.org/doi/full/10.1126/science.abq1158) **Competition-Level Code Generation with AlphaCode.** [![arXiv](https://img.shields.io/badge/arXiv-2203.07814-b31b1b.svg)](https://arxiv.org/abs/2203.07814), 2022.02

   *Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals*

7. [[ICSE2022]](https://dl.acm.org/doi/abs/10.1145/3510003.3510096) **SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations.** [![arXiv](https://img.shields.io/badge/arXiv-2201.01549-b31b1b.svg)](https://arxiv.org/abs/2201.01549), 2021.01

   *Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, Bin Luo* 

8. [[EMNLP2021]](https://aclanthology.org/2021.emnlp-main.685/) **CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2109.00859-b31b1b.svg)](https://arxiv.org/abs/2109.00859), 2021.09

   *Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi* 

9. [[NAACL2021]](https://aclanthology.org/2021.naacl-main.211/) **Unified Pre-training for Program Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2103.06333-b31b1b.svg)](https://arxiv.org/abs/2103.06333), 2021.03

   *Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang* 

### Encoder-Only

1. [Preprint] **SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2108.04556-b31b1b.svg)](https://arxiv.org/abs/2108.04556), 2021.08

   *Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, Xin Jiang* 

2. [[EMNLP2022]](https://aclanthology.org/2022.findings-emnlp.9/) `SCodeR` **Soft-Labeled Contrastive Pre-training for Function-level Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2210.09597-b31b1b.svg)](https://arxiv.org/abs/2210.09597), 2020.10

   *Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng Qiu, Daxin Jiang, Weizhu Chen, Nan Duan* 

3. [[ICLR2021]](https://openreview.net/forum?id=jLoC4ez43PZ) **GraphCodeBERT: Pre-training Code Representations with Data Flow.** [![arXiv](https://img.shields.io/badge/arXiv-2009.08366-b31b1b.svg)](https://arxiv.org/abs/2009.08366), 2020.09

   *Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou* 

4. [[NAACL2022]](https://aclanthology.org/2022.findings-naacl.80/) **CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training.** [![arXiv](https://img.shields.io/badge/arXiv-2205.02029-b31b1b.svg)](https://arxiv.org/abs/2205.02029), 2020.05

   *Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, Jin Liu* 

5. [[EMNLP2020]](https://aclanthology.org/2020.findings-emnlp.139/) **CodeBERT: A Pre-Trained Model for Programming and Natural Languages.** [![arXiv](https://img.shields.io/badge/arXiv-2002.08155-b31b1b.svg)](https://arxiv.org/abs/2002.08155), 2020.02

   *Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou* 

6. [[ICML2020]](https://proceedings.mlr.press/v119/kanade20a.html)**Learning and Evaluating Contextual Embedding of Source Code.** [![arXiv](https://img.shields.io/badge/arXiv-2001.00059-b31b1b.svg)](https://arxiv.org/abs/2001.00059), 2019.12

   *Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi* 


## Others

1. [[EMNLP2023]](https://aclanthology.org/2023.emnlp-main.716/) **CodeFusion: A Pre-trained Diffusion Model for Code Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2310.17680-b31b1b.svg)](https://arxiv.org/abs/2310.17680), 2023.10

   *Mukul Singh, Jos√© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen* 

<!-- 

```bibtex
@misc{sun2024ncisurvey,
  title         = {A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond},
  author        = {Qiushi Sun and Zhirui Chen and Fangzhi Xu and Kanzhi Cheng and Chang Ma and 
                   Zhangyue Yin and Jianing Wang and Chengcheng Han and Renyu Zhu and Shuai Yuan 
                   and Qipeng Guo and Xipeng Qiu and Pengcheng Yin and Xiaoli Li and Fei Yuan and
                   Lingpeng Kong and Xiang Li and Zhiyong Wu},
  eprint        = {2403.14734},
  archivePrefix = {arXiv},
  year          = {2024}
}
‚Äã``` -->

## Acknowledgements

This is an open collaborative research project among:

<a href="https://huggingface.co/OpenAGILab">
    <img src="assets/logos/pjlab-logo.png" alt="Shark-NLP Shanghai AI Lab" height = 50/>
</a>
<a href="https://hkunlp.github.io/">
    <img src="assets/logos/hku_logo.png" alt="Shark-NLP Shanghai AI Lab" height = 50/>
</a>
<a href="https://nus.edu.sg/">
    <img src="assets/logos/logo-high.png" alt="NUS" height = 50/>
</a>
<a href="https://github.com/Shark-NLP">
    <img src="assets/logos/ecnu.svg.png" alt="A*STAR" height = 50/>
</a>
<a href="https://dase.ecnu.edu.cn/">
    <img src="assets/logos/astar-logo.png" alt="A*STAR" height = 50/>
</a>
<a href="https://nlp.fudan.edu.cn/nlpen/main.htm">
    <img src="assets/logos/fudannlp_logo.png" alt="A*STAR" height = 50/>
</a>
<a href="https://deepmind.google/">
    <img src="assets/logos/DeepMind_new_logo.png" alt="A*STAR" height = 50/>
</a>




## Repository Contributors

<a href="qiushisun.github.io"><img src="https://avatars.githubusercontent.com/QiushiSun"  width="50" /></a>
<a href="https://github.com/jet1004"><img src="https://avatars.githubusercontent.com/jet1004"  width="50" /></a>
<a href="https://xufangzhi.github.io/"><img src="https://avatars.githubusercontent.com/xufangzhi"  width="50" /></a>
<a href="https://github.com/Luciferder"><img src="https://avatars.githubusercontent.com/Luciferder"  width="50" /></a>
<a href="https://scholar.google.com/citations?user=9gRQqSkAAAAJ&hl=en"><img src="https://avatars.githubusercontent.com/yinzhangyue"  width="50" /></a>
<a href="https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-CN"><img src="https://avatars.githubusercontent.com/njucckevin"  width="50" /></a>
<a href="https://chang-github-00.github.io/-changma/"><img src="https://avatars.githubusercontent.com/chang-github-00"  width="50" /></a>
<a href="https://hccngu.github.io/"><img src="https://avatars.githubusercontent.com/hccngu"  width="50" /></a>
<a href="https://wjn1996.github.io/"><img src="https://avatars.githubusercontent.com/wjn1996"  width="50" /></a>

<!-- ## Other Good Repos for This Topic -->

## Star History üåü

[![Star History Chart](https://api.star-history.com/svg?repos=QiushiSun/Awesome-Code-Intelligence&type=Date)](https://star-history.com/#QiushiSun/Awesome-Code-Intelligence&Date)

```