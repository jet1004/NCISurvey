<!-- ## A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond -->

## Neural Code Intelligence Survey
This is the repository of our paper: **A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond**.

 <!-- [[Paper](https://qiushisun.github.io/)] -->

[![arXiv](https://img.shields.io/badge/arXiv-2403.14734-b31b1b.svg)](https://arxiv.org/abs/2403.14734) 
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) 
[![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](http://makeapullrequest.com)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![License](https://img.shields.io/badge/License-MIT-blue)
[![Paper page](https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-sm.svg)](https://huggingface.co/papers/2403.14734)
[![Twitter Follow](https://img.shields.io/twitter/follow/qiushi_sun)](https://twitter.com/qiushi_sun)
<!-- [ü§ó](https://huggingface.co/papers/2403.14734) -->

*Please do not hesitate to contact us or launch pull requests if you find any related papers that are missing in our paper.*

## News üì∞
- Update on 2025/01/26: Version 1.6 released with additional topics on code preference learning üìñ
- Update on 2024/11/01: Version 1.4 released with extra paper collections üìñ
- Update on 2024/06/23: Version 1.2 released üöÄ
- Update on 2024/03/19: Version 1.0 released üöÄ
- Update on 2024/01/19: Add multiple reading lists üìñ
- Update on 2023/12/29: Add Development Timelines üìÖ
- Update on 2023/12/25: Add Reading Lists, Merry Christmas üçéüéÑ

## Introduction üìú

üìÉ [**A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond**](https://arxiv.org/abs/2403.14734) 
>
> [Qiushi Sun](https://qiushisun.github.io/),
[Zhirui Chen](https://github.com/jet1004),
[Fangzhi Xu](https://xufangzhi.github.io/),
[Kanzhi Cheng](https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-CN),
[Chang Ma](https://chang-github-00.github.io/-changma/),
[Zhangyue Yin](https://scholar.google.com/citations?user=9gRQqSkAAAAJ&hl=en),
[Jianing Wang](https://wjn1996.github.io/),
[Chengcheng Han](https://hccngu.github.io/),
[Renyu Zhu](https://scholar.google.com/citations?user=tSWULnAAAAAJ&hl=en), 
[Shuai Yuan](https://github.com/Luciferder),
[Qipeng Guo](https://scholar.google.com/citations?user=k3mPGKgAAAAJ&hl=en),
[Xipeng Qiu](https://xpqiu.github.io/),
[Pengcheng Yin](https://pengcheng.in/),
[Xiaoli Li](https://www.a-star.edu.sg/i2r/about-i2r/i2r-management/li-xiaoli), 
[Fei Yuan](https://github.com/CONE-MT), 
[Lingpeng Kong](https://ikekonglp.github.io/), 
[Xiang Li](https://lixiang3776.github.io/), 
[Zhiyong Wu](https://lividwo.github.io/zywu.github.io/)

Introducing the resources provided by our survey paper, slides is also available at [here](assets/NCI_Survey_Slides_V1.pdf).

## Timeline

<details open>
<summary>The Development of Code Intelligence</summary>

![milestones](assets/codelms-tree-v18.png)

</details>


## Recent Work on Code Intelligence (Welcome PR) üìó


- [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316) 2025.2
- [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807) 2025.2
- [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694v1) 2025.1
- [WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models](https://arxiv.org/abs/2412.17395) 2024.12
- [FullStack Bench: Evaluating LLMs as Full Stack Coders](https://arxiv.org/abs/2412.00535) 2024.12
- [CodeDPO: Aligning Code Models with Self Generated and Verified Source Code](https://arxiv.org/abs/2410.05605) 2024.11
- [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://opencoder-llm.github.io/) 2024.11
- [Qwen2.5-Coder Series: Powerful, Diverse, Practical.](https://github.com/QwenLM/Qwen2.5-Coder) 2024.11
<!-- - [RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning](https://huggingface.co/papers/2410.02089) 2024.10 -->
<!-- - [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914) 2024.08 -->
  

<!-- - [Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](https://arxiv.org/abs/2406.11736) 2024.06
- [Symbolic Learning Enables Self-Evolving Agents](https://arxiv.org/pdf/2406.18532v1) 2024.06
- [aiXcoder](https://huggingface.co/aiXcoder/aixcoder-7b-base) 2024.04
- [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) 2024.03 -->


## Paper Collections / Tutorials üìö

- [Language Models for Code](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/CodeLMs.md) ü§ñ
- [Evaluations and Benchmarks](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Benchmarks.md) üìä 
- [Preference Optimization](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Preference-Optimization.md) üçé
- [Code Repair](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Repair.md) üîß
- [Reasoning with Code Synthesis](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Reasoning.md) üß†
- [Data Science](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/DS.md) üî¢
- [Corpus containing Code Data](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Code-corpus.md) üìö
- [Code-Based Solutions for NLP Tasks](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/NLPTasks-through-code.md) üìù
- [Code Empowered Agents](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/CodeLM-empowered-agents.md) ü§ñ
- [Reinforcement Learning with CodeLMs](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/RL-with-CodeLMs.md) üéÆ
- [Code Intelligence assists AI4Science](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/AI4Science.md) üß™
- [Software Development](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Software-Development.md) üõ†Ô∏è
- [Multilingual](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/multilingual.md) üåç
- [Multimodal Code Generation](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/Multimodal.md) üé®
- [Awesome Slides, Talks and Blogs](https://github.com/QiushiSun/NCISurvey/blob/main/paper-reading/tutorials.md) üßë‚Äçüè´

## Citation üìñ

ü´∂ If you are interested in our work or find this repository helpful, please consider using the following citation format when referencing our paper:

```bibtex
@article{sun2024survey,
  title={A survey of neural code intelligence: Paradigms, advances and beyond},
  author={Sun, Qiushi and Chen, Zhirui and Xu, Fangzhi and Cheng, Kanzhi and Ma, Chang and Yin, Zhangyue and Wang, Jianing and Han, Chengcheng and Zhu, Renyu and Yuan, Shuai and others},
  journal={arXiv preprint arXiv:2403.14734},
  year={2024}
}
```
## Table of Contents

1. [Language Models for Code](#CodeLMs)
   - [CodeLLMs / Decoder-Only](#CodeLLMs)
   - [Encoder-Decoder](#enc-dec)
   - [Encoder-Only](#enc)
   - [Others](#others)

## Language Models for Code <a id="CodeLMs"></a>

### CodeLLMs / Decoder-Only <a id="CodeLLMs"></a>

1. [Preprint] **Qwen2.5-Coder Series: Powerful, Diverse, Practical.** [[Website]](https://github.com/QwenLM/Qwen2.5-Coder), 2024.11
2. [Preprint] **OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models** [[Website]](https://opencoder-llm.github.io/) [![arXiv](https://img.shields.io/badge/arXiv-2411.04905-b31b1b.svg)](https://arxiv.org/abs/2411.04905), 2024.11

3. [[COLM2024]](https://openreview.net/forum?id=kWnlCVcp6o#discussion) **Crystal: Illuminating LLM Abilities on Language and Code** [![arXiv](https://img.shields.io/badge/arXiv-2411.04156-b31b1b.svg)](https://arxiv.org/abs/2411.04156), 2024.10
4. [Preprint]  [[Website]](https://github.com/aixcoder-plugin/aiXcoder-7B)
5. [Preprint] **CodeGeeX4: Open Multilingual Code Generation Model.** [[Repo]](https://github.com/THUDM/CodeGeeX4), 2024.07
6. [Preprint] **CodeGemma: Open Code Models Based on Gemmaa.** [![arXiv](https://img.shields.io/badge/arXiv-2406.11409-b31b1b.svg)](https://arxiv.org/abs/2406.11409), 2024.06
7. [Preprint] **Codestral** [[Website]](https://mistral.ai/news/codestral/), 2024.05
8. [Preprint] **Code with CodeQwen1.5** [[Blog Post]](https://qwenlm.github.io/blog/codeqwen1.5/), 2024.04
9. [Preprint] **StarCoder 2 and The Stack v2: The Next Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2401.14196-b31b1b.svg)](https://arxiv.org/abs/2402.19173), 2024.02
10. [Preprint] **DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence.** [![arXiv](https://img.shields.io/badge/arXiv-2401.14196-b31b1b.svg)](https://arxiv.org/abs/2401.14196), 2024.01
11. [Preprint]  [![arXiv](https://img.shields.io/badge/arXiv-2311.09278-b31b1b.svg)](https://arxiv.org/abs/2311.09278), 2023.11 [![zhihu](https://img.shields.io/badge/dynamic/json?label=upvotes&style=social&logo=zhihu&query=$.voteup_count&url=https://www.zhihu.com/api/v4/articles/690058207)](https://zhuanlan.zhihu.com/p/690058207)
12.  **Llemma: An Open Language Model For Mathematics.** [![arXiv](https://img.shields.io/badge/arXiv-2310.10631-b31b1b.svg)](https://arxiv.org/abs/2310.10631), 2023.10
13.  **Lemur: Harmonizing Natural Language and Code for Language Agents.** [![arXiv](https://img.shields.io/badge/arXiv-2310.06830-b31b1b.svg)](https://arxiv.org/abs/2310.06830), 2023.10
14. [Preprint] **ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.** [![arXiv](https://img.shields.io/badge/arXiv-2309.17452-b31b1b.svg)](https://arxiv.org/abs/2309.17452), 2023.09
15. [Preprint] **MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning.** [![arXiv](https://img.shields.io/badge/arXiv-2309.05653-b31b1b.svg)](https://arxiv.org/abs/2309.05653), 2023.09
16. [Preprint] `phi-1.5` **Textbooks Are All You Need II: phi-1.5 technical report.** [![arXiv](https://img.shields.io/badge/arXiv-2309.05463-b31b1b.svg)](https://arxiv.org/abs/2309.05463), 2023.09
17. [Preprint] **Code Llama: Open Foundation Models for Code.** [![arXiv](https://img.shields.io/badge/arXiv-2308.12950-b31b1b.svg)](https://arxiv.org/abs/2308.12950), 2023.08
18. [Preprint] `OctoCoder` **OctoPack: Instruction Tuning Code Large Language Models.** [![arXiv](https://img.shields.io/badge/arXiv-2308.07124-b31b1b.svg)](https://arxiv.org/abs/2308.07124), 2023.08
19. [Preprint] **PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback.** [![arXiv](https://img.shields.io/badge/arXiv-2307.14936-b31b1b.svg)](https://arxiv.org/abs/2307.14936), 2023.07
20. [Preprint] `phi-1` **Textbooks Are All You Need.** [![arXiv](https://img.shields.io/badge/arXiv-2306.11644-b31b1b.svg)](https://arxiv.org/abs/2306.11644), 2023.06
21. [Preprint] **WizardCoder: Empowering Code Large Language Models with Evol-Instruct.** [![arXiv](https://img.shields.io/badge/arXiv-2306.08568-b31b1b.svg)](https://arxiv.org/abs/2306.08568), 2023.06
22.  **StarCoder: may the source be with you!** [![arXiv](https://img.shields.io/badge/arXiv-2305.06161-b31b1b.svg)](https://arxiv.org/abs/2305.06161), 2023.05
23.  **CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.** [![arXiv](https://img.shields.io/badge/arXiv-2303.17568-b31b1b.svg)](https://arxiv.org/abs/2303.17568), 2023.03
24. [Preprint] **SantaCoder: don't reach for the stars!** [![arXiv](https://img.shields.io/badge/arXiv-2301.03988-b31b1b.svg)](https://arxiv.org/abs/2301.03988), 2023.01
25. [Preprint] `FIM` **Efficient Training of Language Models to Fill in the Middle.** [![arXiv](https://img.shields.io/badge/arXiv-2207.14255-b31b1b.svg)](https://arxiv.org/abs/2207.14255), 2022.07
26. [Preprint] **PanGu-Coder: Program Synthesis with Function-Level Language Modeling.** [![arXiv](https://img.shields.io/badge/arXiv-2207.11280-b31b1b.svg)](https://arxiv.org/abs/2207.11280), 2022.07
27.  `PyCodeGPT` **CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation** [![arXiv](https://img.shields.io/badge/arXiv-2206.06888-b31b1b.svg)](https://arxiv.org/abs/2206.06888), 2022.06
28.  **InCoder: A Generative Model for Code Infilling and Synthesis.** [![arXiv](https://img.shields.io/badge/arXiv-2204.05999-b31b1b.svg)](https://arxiv.org/abs/2204.05999), 2022.04
29.  `PaLM-Coder` **PaLM: Scaling Language Modeling with Pathways.** [![arXiv](https://img.shields.io/badge/arXiv-2204.02311-b31b1b.svg)](https://arxiv.org/abs/2204.02311), 2022.04
30.  **CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis.** [![arXiv](https://img.shields.io/badge/arXiv-2203.13474-b31b1b.svg)](https://arxiv.org/abs/2203.13474), 2022.03
31.  `PolyCoder` **A Systematic Evaluation of Large Language Models of Code.** [![arXiv](https://img.shields.io/badge/arXiv-2202.13169-b31b1b.svg)](https://arxiv.org/abs/2202.13169), 2022.02
32. [Preprint] `Code-davinci-002` **Evaluating Large Language Models Trained on Code.** [![arXiv](https://img.shields.io/badge/arXiv-2107.03374-b31b1b.svg)](https://arxiv.org/abs/2107.03374), 2021.7
33. [[ESEC/FSE2020]](https://2020.esec-fse.org/details/esecfse-2020-industry-papers/13/IntelliCode-Compose-Code-Generation-using-Transformer) `GPT-C` **IntelliCode Compose: Code Generation using Transformer.** [![arXiv](https://img.shields.io/badge/arXiv-2005.08025-b31b1b.svg)](https://arxiv.org/abs/2005.08025), 2020.05

### Encoder-Decoder <a id="enc-dec"></a>

1. [[EMNLP2023]](https://aclanthology.org/2023.emnlp-main.68/) **CodeT5+: Open Code Large Language Models for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2305.07922-b31b1b.svg)](https://arxiv.org/abs/2305.07922), 2023.05

2. [[ACL2023]](https://aclanthology.org/2023.findings-acl.676/) **ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages.** [![arXiv](https://img.shields.io/badge/arXiv-2212.06742-b31b1b.svg)](https://arxiv.org/abs/2212.06742), 2022.12

3. [[NeurIPS2022]](https://openreview.net/forum?id=WaGvb7OzySA) **CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning.** [![arXiv](https://img.shields.io/badge/arXiv-2207.01780-b31b1b.svg)](https://arxiv.org/abs/2207.01780), 2022.07

4. [[ESEC/FSE]](https://dl.acm.org/doi/abs/10.1145/3540250.3549162) **NatGen: Generative pre-training by "Naturalizing" source code.** [![arXiv](https://img.shields.io/badge/arXiv-2206.07585-b31b1b.svg)](https://arxiv.org/abs/2206.07585), 2022.06

5. [[ACL2022]](https://aclanthology.org/2022.acl-long.499/) **UniXcoder: Unified Cross-Modal Pre-training for Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2203.03850-b31b1b.svg)](https://arxiv.org/abs/2203.03850), 2022.03

6. [[Science]](https://www.science.org/doi/full/10.1126/science.abq1158) **Competition-Level Code Generation with AlphaCode.** [![arXiv](https://img.shields.io/badge/arXiv-2203.07814-b31b1b.svg)](https://arxiv.org/abs/2203.07814), 2022.02

7. [[ICSE2022]](https://dl.acm.org/doi/abs/10.1145/3510003.3510096) **SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations.** [![arXiv](https://img.shields.io/badge/arXiv-2201.01549-b31b1b.svg)](https://arxiv.org/abs/2201.01549), 2021.01

8. [[EMNLP2021]](https://aclanthology.org/2021.emnlp-main.685/) **CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2109.00859-b31b1b.svg)](https://arxiv.org/abs/2109.00859), 2021.09

9. [[NAACL2021]](https://aclanthology.org/2021.naacl-main.211/) **Unified Pre-training for Program Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2103.06333-b31b1b.svg)](https://arxiv.org/abs/2103.06333), 2021.03

### Encoder-Only <a id="enc"></a>

1. [Preprint] **SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2108.04556-b31b1b.svg)](https://arxiv.org/abs/2108.04556), 2021.08

2. [[EMNLP2022]](https://aclanthology.org/2022.findings-emnlp.9/) `SCodeR` **Soft-Labeled Contrastive Pre-training for Function-level Code Representation.** [![arXiv](https://img.shields.io/badge/arXiv-2210.09597-b31b1b.svg)](https://arxiv.org/abs/2210.09597), 2020.10

3. [[ICLR2021]](https://openreview.net/forum?id=jLoC4ez43PZ) **GraphCodeBERT: Pre-training Code Representations with Data Flow.** [![arXiv](https://img.shields.io/badge/arXiv-2009.08366-b31b1b.svg)](https://arxiv.org/abs/2009.08366), 2020.09

4. [[NAACL2022]](https://aclanthology.org/2022.findings-naacl.80/) **CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training.** [![arXiv](https://img.shields.io/badge/arXiv-2205.02029-b31b1b.svg)](https://arxiv.org/abs/2205.02029), 2020.05

5. [[EMNLP2020]](https://aclanthology.org/2020.findings-emnlp.139/) **CodeBERT: A Pre-Trained Model for Programming and Natural Languages.** [![arXiv](https://img.shields.io/badge/arXiv-2002.08155-b31b1b.svg)](https://arxiv.org/abs/2002.08155), 2020.02

6. [[ICML2020]](https://proceedings.mlr.press/v119/kanade20a.html)**Learning and Evaluating Contextual Embedding of Source Code.** [![arXiv](https://img.shields.io/badge/arXiv-2001.00059-b31b1b.svg)](https://arxiv.org/abs/2001.00059), 2019.12


## Others <a id="others"></a>

1. [[EMNLP2023]](https://aclanthology.org/2023.emnlp-main.716/) **CodeFusion: A Pre-trained Diffusion Model for Code Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2310.17680-b31b1b.svg)](https://arxiv.org/abs/2310.17680), 2023.10

   

<!-- 

```bibtex
@misc{sun2024ncisurvey,
  title         = {A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond},
  author        = {Qiushi Sun and Zhirui Chen and Fangzhi Xu and Kanzhi Cheng and Chang Ma and 
                   Zhangyue Yin and Jianing Wang and Chengcheng Han and Renyu Zhu and Shuai Yuan 
                   and Qipeng Guo and Xipeng Qiu and Pengcheng Yin and Xiaoli Li and Fei Yuan and
                   Lingpeng Kong and Xiang Li and Zhiyong Wu},
  eprint        = {2403.14734},
  archivePrefix = {arXiv},
  year          = {2024}
}
‚Äã``` -->

## Acknowledgements

This is an open collaborative research project among:

<a href="https://huggingface.co/OpenAGILab">
    <img src="assets/logos/pjlab-logo.png" alt="Shark-NLP Shanghai AI Lab" height = 50/>
</a>
<a href="https://hkunlp.github.io/">
    <img src="assets/logos/hku_logo.png" alt="Shark-NLP Shanghai AI Lab" height = 50/>
</a>
<a href="https://nus.edu.sg/">
    <img src="assets/logos/logo-high.png" alt="NUS" height = 50/>
</a>
<a href="https://github.com/Shark-NLP">
    <img src="assets/logos/ecnu.svg.png" alt="A*STAR" height = 50/>
</a>
<a href="https://dase.ecnu.edu.cn/">
    <img src="assets/logos/astar-logo.png" alt="A*STAR" height = 50/>
</a>
<a href="https://nlp.fudan.edu.cn/nlpen/main.htm">
    <img src="assets/logos/fudannlp_logo.png" alt="A*STAR" height = 50/>
</a>
<a href="https://deepmind.google/">
    <img src="assets/logos/DeepMind_new_logo.png" alt="A*STAR" height = 50/>
</a>




## Repository Contributors

<a href="qiushisun.github.io"><img src="https://avatars.githubusercontent.com/QiushiSun"  width="50" /></a>
<a href="https://github.com/jet1004"><img src="https://avatars.githubusercontent.com/jet1004"  width="50" /></a>
<a href="https://xufangzhi.github.io/"><img src="https://avatars.githubusercontent.com/xufangzhi"  width="50" /></a>
<a href="https://github.com/Luciferder"><img src="https://avatars.githubusercontent.com/Luciferder"  width="50" /></a>
<a href="https://scholar.google.com/citations?user=9gRQqSkAAAAJ&hl=en"><img src="https://avatars.githubusercontent.com/yinzhangyue"  width="50" /></a>
<a href="https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-CN"><img src="https://avatars.githubusercontent.com/njucckevin"  width="50" /></a>
<a href="https://chang-github-00.github.io/-changma/"><img src="https://avatars.githubusercontent.com/chang-github-00"  width="50" /></a>
<a href="https://hccngu.github.io/"><img src="https://avatars.githubusercontent.com/hccngu"  width="50" /></a>
<a href="https://wjn1996.github.io/"><img src="https://avatars.githubusercontent.com/wjn1996"  width="50" /></a>

<!-- ## Other Good Repos for This Topic -->

## Star History üåü

[![Star History Chart](https://api.star-history.com/svg?repos=QiushiSun/Awesome-Code-Intelligence&type=Date)](https://star-history.com/#QiushiSun/Awesome-Code-Intelligence&Date)

```